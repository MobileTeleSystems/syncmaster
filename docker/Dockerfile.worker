ARG PYTHON_VERSION=3.13
FROM python:$PYTHON_VERSION-slim-bookworm AS base

RUN apt-get update && apt-get install -y --no-install-recommends \
        openjdk-17-jdk-headless \
        # required for HDFS/Hive with Kerberos enabled
        krb5-user \
    && rm -rf /var/lib/apt/lists/* /var/cache/*

WORKDIR /app
ENV PYTHONPATH=/app \
    PATH="/app/.venv/bin:$PATH" \
    POETRY_VIRTUALENVS_IN_PROJECT=1 \
    POETRY_VIRTUALENVS_CREATE=1

COPY ./docker/entrypoint_worker.sh /app/entrypoint.sh
RUN chmod +x /app/entrypoint.sh
ENTRYPOINT ["/app/entrypoint.sh"]
CMD ["--loglevel=info"]


FROM base AS builder

RUN apt-get update && apt-get install -y --no-install-recommends \
        autoconf \
        gcc \
        make \
        # required to build gssapi from sources
        libkrb5-dev \
    && rm -rf /var/lib/apt/lists/* /var/cache/*

RUN --mount=type=cache,target=/root/.cache/pip \
    pip install poetry

COPY ./pyproject.toml ./poetry.lock ./
RUN --mount=type=cache,target=/root/.cache/pypoetry \
    poetry install \
        --no-root \
        --extras "worker" \
        --without test,docs,dev \
    && python -m compileall -j 4 .venv


FROM builder AS maven_packages

RUN --mount=type=bind,source=./syncmaster/,target=/app/syncmaster/ \
    --mount=type=bind,source=./docker/download_maven_packages.py,target=/app/docker/download_maven_packages.py \
    mkdir /root/.ivy2 && \
    # Try to download all dependencies at once.
    # If multiple packages depends on the same transitive dependency, Spark uses maximum version of this dependency.
    python /app/docker/download_maven_packages.py all && \
    # Then try to download specific connectors to fetch exact dependency version specified within connector.
    # Yes, this is slow, but overwise using worker without internet access will fail, unless custom ivysettings.xml is used
    python /app/docker/download_maven_packages.py s3 && \
    python /app/docker/download_maven_packages.py hdfs && \
    python /app/docker/download_maven_packages.py clickhouse && \
    python /app/docker/download_maven_packages.py postgres && \
    python /app/docker/download_maven_packages.py oracle && \
    python /app/docker/download_maven_packages.py mssql && \
    python /app/docker/download_maven_packages.py mysql
    # if someone uses custom worker image, they should download jars on their own


FROM base AS prod

# Do not run production as root, to improve security.
# Also user does not own anything inside the image, including venv and source code.
RUN useradd syncmaster && \
    mkdir -p /home/syncmaster /home/syncmaster/.ivy2 && \
    chown -R syncmaster:syncmaster /home/syncmaster

# We don't need poetry and compilers in final image
COPY --from=builder /app/.venv/ /app/.venv/
# custom Spark session function may download different jars, so syncmaster have to own them
COPY --from=maven_packages --chown=syncmaster:syncmaster /root/.ivy2/ /home/syncmaster/.ivy2/

COPY ./syncmaster/ /app/syncmaster/
RUN python -m compileall syncmaster
USER syncmaster


FROM builder AS test

RUN --mount=type=cache,target=/root/.cache/pypoetry \
    poetry install \
        --no-root \
        # CI runs tests in the worker container,
        # so we need server & scheduler dependencies too
        --all-extras \
        --with test \
        --without docs,dev \
    && python -m compileall -j 4 .venv

ENV SYNCMASTER__WORKER__CREATE_SPARK_SESSION_FUNCTION=tests.spark.get_worker_spark_session

# Collect coverage from worker
RUN sed -i 's/python -m/coverage run -m/g' /app/entrypoint.sh

# Replace kinit binary with dummy, to skip Kerberos interaction in tests
RUN mkdir -p /app/.local/bin && \
    echo "#!/bin/bash" > /app/.local/bin/kinit \
    && chmod +x /app/.local/bin/kinit
ENV PATH="/app/.local/bin:$PATH"
